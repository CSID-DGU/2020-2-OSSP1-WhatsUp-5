{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GENERATE_MODEL.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_CHZgDiWSi-6"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKRJhg2JSkBH"},"source":[" cd /content/drive/Shareddrives/와썹_공개SW/GPT2_MNews/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOXgUZV8TA6w"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1A8Mz94jTCga"},"source":["import os\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","import gluonnlp as nlp\n","from gluonnlp.data import SentencepieceTokenizer\n","from transformers import TFGPT2LMHeadModel\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from nltk.tokenize import sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDBGvKX3UKxb"},"source":["class GPT2Model(tf.keras.Model):\n","    def __init__(self, dir_path):\n","        super(GPT2Model, self).__init__()\n","        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n","        \n","    def call(self, inputs):\n","        return self.gpt2(inputs)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xWXq7x1U0Zc"},"source":["BATCH_SIZE = 16\n","NUM_EPOCHS = 10\n","MAX_LEN = 100\n","TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","\n","tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                               mask_token=None,\n","                                               sep_token=None,\n","                                               cls_token=None,\n","                                               unknown_token='<unk>',\n","                                               padding_token='<pad>',\n","                                               bos_token='<s>',\n","                                               eos_token='</s>')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"maBeNLrdUOlW"},"source":["BASE_MODEL_PATH = './data_out_11_29_2/tf2_gpt2_finetuned_model'\n","gpt_model = GPT2Model(BASE_MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYn1kze9SvH9"},"source":["def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n","    _logits = logits.numpy()\n","    top_k = min(top_k, logits.shape[-1])  \n","    if top_k > 0:\n","        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n","        _logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits = tf.sort(logits, direction='DESCENDING')\n","        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n","        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n","\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n","        \n","        _logits[indices_to_remove] = filter_value\n","    return tf.constant([_logits])\n","\n","\n","def generate_sent(seed_word, model, max_step=500, greedy=False, top_k=0, top_p=0.):\n","    sent = seed_word\n","    toked = tokenizer(sent)\n","    for _ in range(max_step):\n","        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n","        outputs = model(input_ids)[:, -1, :]\n","        if greedy:\n","            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n","        else:\n","            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n","            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n","        if gen == '</s>':\n","            break\n","        sent += gen.replace('▁', ' ')\n","        toked = tokenizer(sent)\n","    return sent"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhxcVbmlpggC"},"source":["# 문장 생성"]},{"cell_type":"code","metadata":{"id":"yKRGbBRZS1mu"},"source":["generate_sent('당뇨', gpt_model,top_k=40, top_p=0.90)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvDK_-zMpNYX"},"source":["generate_sent('당뇨란', gpt_model,top_k=0, top_p=0.90)"],"execution_count":null,"outputs":[]}]}