{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_finetuning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSID-DGU/2020-2-OSSP1-WhatsUp-5/blob/master/model/GPT2_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_jSucTag2t",
        "outputId": "2046784c-391f-498b-88a1-fbf2fe55a9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoGPT2'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 100 (delta 52), reused 56 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (100/100), 548.66 KiB | 19.59 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ra1de4JcX1e",
        "outputId": "78e9ccb7-8706-4bc4-ca75-a4e8225afeb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd KoGPT2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JB20fpLabZz"
      },
      "source": [
        "기존의 SKT-AI의 환경설정 sentencepiece >= 0.1.85를 sentencepiece == 0.1.85로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWPvIeGgcYv0",
        "outputId": "c8b610e9-cb22-452c-8fd3-7d58dcdbfddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 8.8MB/s \n",
            "\u001b[?25hCollecting mxnet==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 45kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.85\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.5MB/s \n",
            "\u001b[?25hCollecting torch==1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0MB 17kB/s \n",
            "\u001b[?25hCollecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (0.17.0)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470037 sha256=60cf1df81658395efb44407a91a3eef86784e363b35dcb91b2b4d0894b36af12\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=215150bd6b0f350ade4d5eb5525e5bb6df94a48cd3d50794444175b6de0d5ebf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp sacremoses\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.5.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, torch, tokenizers, sacremoses, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.7.0 torch-1.5.0 transformers-2.11.0\n",
            "Processing /content/KoGPT2\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14054 sha256=a4594784f1089858a02cc5a062ba64c3486c3f2e8878f49ac681491b371e2a9d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e0svv6t6/wheels/ac/e5/83/e839e6a987261c05b2e32cbd9770007e19f8ea7e2f2f7b9d3c\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcKQlRIufDt3"
      },
      "source": [
        "# 허깅페이스의 transformers 라이브러리와 SKT KoGPT2 모델 기반으로 작성된 코드\n",
        "---환경 설정 때문에 위에서 skt꺼 import 함\n",
        "--추후에는 해당 git에 있는 requirements.txt만 임포트 하면 될 것이라고 생각함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjx93wf8capL"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWc_hl3-e6m8"
      },
      "source": [
        "\n",
        "모델을 학습하기 위해 필요한 3까지 모듈\n",
        "1. TFGPT2LMHeadModel: 문장 생성\n",
        "2. gluonnlp의 SentencepieceTokenizer\n",
        "3. nlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-brv0bReLyZ"
      },
      "source": [
        "\n",
        "class GPT2Model(tf.keras.Model):\n",
        "    def __init__(self, dir_path):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        return self.gpt2(inputs)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I-lC513ryzi"
      },
      "source": [
        "__init__ 함수에서 TFGPT2LMHeadModel을 생성해서 실행할 수 있게 구현함.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# self.gpt2(inputs)[0]\n",
        "```\n",
        "\n",
        "생성모델을 활용하기 위해서는 vocabulary에 대한 logit 값만 활용하도록 첫 번째 값인 last_hidden_states 출력\n",
        "\n",
        "--> 3-d pytorch_kogpt2.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwCL_xYUgISb",
        "outputId": "335c9e39-d5e0-423a-b1e0-7ad81a4741b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "!unzip -o gpt_ckpt.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 07:59:48--  https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/223835896/5fc78d00-1783-11eb-8790-e30a33628113?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201105T075948Z&X-Amz-Expires=300&X-Amz-Signature=2f0a47ed95c67bf766319568c6ea0d5a9563c78463693b2638a84c5f2ab5c74d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=223835896&response-content-disposition=attachment%3B%20filename%3Dgpt_ckpt.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-11-05 07:59:48--  https://github-production-release-asset-2e65be.s3.amazonaws.com/223835896/5fc78d00-1783-11eb-8790-e30a33628113?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201105T075948Z&X-Amz-Expires=300&X-Amz-Signature=2f0a47ed95c67bf766319568c6ea0d5a9563c78463693b2638a84c5f2ab5c74d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=223835896&response-content-disposition=attachment%3B%20filename%3Dgpt_ckpt.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.238.107\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.238.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460908853 (440M) [application/octet-stream]\n",
            "Saving to: ‘gpt_ckpt.zip’\n",
            "\n",
            "gpt_ckpt.zip        100%[===================>] 439.56M  65.5MB/s    in 7.2s    \n",
            "\n",
            "2020-11-05 07:59:56 (60.7 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
            "\n",
            "Archive:  gpt_ckpt.zip\n",
            "   creating: gpt_ckpt/\n",
            "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
            "  inflating: gpt_ckpt/config.json    \n",
            "  inflating: gpt_ckpt/tf_model.h5    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtFR8N90R2BQ"
      },
      "source": [
        "**학습 파라미터 내려받고 준비하기**\n",
        "\n",
        "학습된 파라미터가 koGPT2의 경우 huggingface에 모델로 등록돼 있지 않아 파라미터를 다운로드 해야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMjhUSAGeXF6"
      },
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1GKb0rHSPLQ"
      },
      "source": [
        "모델 리소스 경로를 객체를 생성할 때 인자로 전달하여 학습된 파라미터를 가지는 GPT2 모델 객체 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hIfiaBAS5v-"
      },
      "source": [
        "## 사전 학습된 모델을 활용해 언어 생성 결과 확인\n",
        "\n",
        "단어 하나가 주어지면 문장을 만들어주는 방식으로\n",
        "\n",
        "1105 변경사항: `tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llviuFQ6esS_"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "MAX_LEN = 30\n",
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, num_best=0, alpha=0)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token=None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk8bzPM4TDOQ"
      },
      "source": [
        "**토크나이저 생성**\n",
        "\n",
        "텍스트를 모델에 입력하려면 필요함. \n",
        "앞서 불러온 SentencepieceTokenizer와 nlp 모듈의 vocab 활용하여 단어 사전과 토크나이저 정의.\n",
        "\n",
        "\n",
        "> GPT2의 각 스페셜 토큰의 역할\n",
        "\n",
        "1.   <unk<unk>> 모르는 단어에 대한 토큰\n",
        "2.   <pad<pad>> 배치 데이터 길이 맞추는 용도\n",
        "3.   <s<s>> 문장의 시작을 알림\n",
        "4.   </s<s>> 문장의 종결을 알림\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUOBGwjDi-kU"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "    _logits = logits.numpy()\n",
        "    top_k = min(top_k, logits.shape[-1])  \n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "        _logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "        \n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    return tf.constant([_logits])\n",
        "\n",
        "\n",
        "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "    sent = seed_word\n",
        "    toked = tokenizer(sent)\n",
        "    \n",
        "    for _ in range(max_step):\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :]\n",
        "        if greedy:\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "        else:\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "        if gen == '</s>':\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnVpcWsgVXT1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "```\n",
        "-see_word: 문장 생성의 시작 단어\n",
        "\n",
        "-model: 문장 생성을 수행할 모델\n",
        "\n",
        "-max_step: 생성 횟수를 제한\n",
        "\n",
        "-greedy: 모델 출력 결과에 대해 유연하게 문장 생성을 해줄 수 있는지 선택할 수 있도록\n",
        "\n",
        "*   greedy=true: 문장 출력 결과에 대해 가장 확률이 높은 단어만 선택\n",
        "*   greedy=false: 출력한 단어 가운데 확률 또는 순위가 높은 단어만 선택해 무작위 생성\n",
        "\n",
        "+) top_k와 top_p 파라미터:  false인 경우 사용. top_k는 확률이 높은 순새대로 k번째까지 필터링. top_p는 일정 확률값 이상인 단어에 대해 필터링\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  sent = seed_word\n",
        "   toked = tokenizer(sent)\n",
        "```\n",
        "\n",
        "문장 시작 단어를 변수에 할당하고 토크나이즈\n",
        "\n",
        "\n",
        "```\n",
        "# for _ in range(max_step):\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :]\n",
        "```\n",
        "문장 생성을 할 수 있는 반복문. 토크나이즈된 단어를 인덱스로 변환하고 모델에 입력값으로 넣어 출력값을 받음. 모델의 출력값에 대해서는 문장에서 마지막 단어만 선택\n",
        "\n",
        "```\n",
        "# if gen == '</s>':\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "```\n",
        "생성된 텍스트 토큰이 문장의 끝을 알리는 </s</s>>토큰이면 생성 stop하고 앞서 만들어진 텍스트에 덧붙임\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8BB75iNtwEs"
      },
      "source": [
        "### **테스트-미세 조정 전**\n",
        "\n",
        "1.greedy 방식: 확률이 가장 높은 단어만 선택, 학습한 바이어스에 따라 일관된 문장만 출력, 반복되는 단어가 출력되는 결과가 나올 수도 있음\n",
        "\n",
        "2. 샘플링 방식: 좀 더 자연스러움"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfL6bgrnjFo_",
        "outputId": "f9ee042e-9112-4382-9cfb-500116a9e073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, greedy=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때문에 일부 전문가들은 “이번 사건은 ‘제2의 삼성’을 꿈꾸는 삼성의 내부 사정을 잘 보여주는 사례”라고 평가했다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT0k9ZDckGgs",
        "outputId": "f441325e-ca60-4441-a90d-84faaf056bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때 가끔 떠오르는 ‘로츠맨’ 이라는 별명만큼 유쾌한 얼굴이 아니라, 의외의 존재감으로 새침한 얼굴에 담근 진지함이 묻어나는 웃음 포인트가 한결 밝아졌다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpd17unbkuF_"
      },
      "source": [
        "# 소설 텍스트 데이터 전처리하기\n",
        "\n",
        "미세 조정할 학습 데이터 구성---학습할 데이터는 운수 좋은 날"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5oOt3j8kPu0",
        "outputId": "cdda038b-6518-4004-b1dc-52bdec39aaa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/NLP-kr/tensorflow-ml-nlp-tf2.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-ml-nlp-tf2'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 1544 (delta 57), reused 14 (delta 4), pack-reused 1443\u001b[K\n",
            "Receiving objects: 100% (1544/1544), 200.95 MiB | 28.81 MiB/s, done.\n",
            "Resolving deltas: 100% (929/929), done.\n",
            "Checking out files: 100% (83/83), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PdLt4mHlzwX",
        "outputId": "c305230e-babf-4f34-95f5-6a53b0668233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd tensorflow-ml-nlp-tf2/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2/tensorflow-ml-nlp-tf2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00KpWfpYo8ty",
        "outputId": "70c8650c-23ed-4bee-b63a-8cba0e89a64b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd 7.PRETRAIN_METHOD"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2/tensorflow-ml-nlp-tf2/7.PRETRAIN_METHOD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC8yDDVFpGRh"
      },
      "source": [
        "DATA_IN_PATH = './data_in/KOR/'\n",
        "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
        "\n",
        "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE).readlines()]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q41FEzHIB74E"
      },
      "source": [
        "학습데이터는 소설 텍스트를 먼저 문장별로 분리해둔 텍스트 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0DTlbHvpMuR",
        "outputId": "fc77b381-39a9-4b93-f800-4d13b2ca88de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_data = []\n",
        "output_data = []\n",
        "\n",
        "for s in sents:\n",
        "    print(s)\n",
        "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
        "    input_data.append(tokens[:-1])\n",
        "    output_data.append(tokens[1:])\n",
        "\n",
        "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "\n",
        "input_data = np.array(input_data, dtype=np.int64)\n",
        "output_data = np.array(output_data, dtype=np.int64)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그때에 김첨지는 대수롭지 않은듯이,\n",
            "만일 김첨지가 주기를 띠지 않았던들 한 발을 대문에 들여놓았을 제 그곳을 지배하는 무시무시한 정적(靜寂) ― 폭풍우가 지나간 뒤의 바다 같은 정적이 다리가 떨렸으리라.\n",
            "마음대로 할 양이면 거기 있는 모든 먹음먹이를 모조리 깡그리 집어삼켜도 시원치 않았다 하되 배고픈 이는 위선 분량 많은 빈대떡 두 개를 쪼이기도 하고 추어탕을 한 그릇청하였다.\n",
            "하고 추근추근하게도 그 여자의 들고 있는 일본식 버들고리짝에 제 손을 대었다.\n",
            "흡뜬 눈은 조금 바루어졌건만 이슬이 맺히었다.\n",
            "김첨지는 교묘하게도 정말 꾀꼬리 같은 소리를 내었다.\n",
            "하고 어린애 모양으로 손뼉을 치며 웃는다.\n",
            "응아 소리도 입에서 나는 게 아니고 마치 뱃속에서 나는 듯하였다.\n",
            "그러자 그 돈벌 용기가 병자에 대한 염려를 사르고 말았다.\n",
            "“설렁탕을 사다 놓았는데 왜 먹지를 못하니, 왜 먹지를 못하니…… 괴상하게도 오늘은!\n",
            "제발 덕분에 집에 붙어 있어요.\n",
            "한꺼번에 이런 금액을 불러라도 본 지가 그 얼마 만인가!\n",
            "기어이 일 원 어치를 채워서 곱배기 한 잔씩 더 먹고 나왔다.\n",
            "기적(奇蹟)\n",
            "거기 마침 마마님이신지 여학생이신지 (요새야 어디 논다니와 아가씨를 구별할 수가 있던가) 망토를 잡수시고 비를 맞고 서 있겠지.\n",
            "그래 그는 이전에도 여러 번 해본 일이라 바로 정거장 앞 전차 정류장에서 조금 떨어지게 사람 다니는 길과 전찻길 틈에 인력거를 세워 놓고 자기는 그 근처를 빙빙 돌며 형세를 관망하기로 하였다.\n",
            "하면서 풀매질을 친다.\n",
            "이런 말을 하며 일변 돈을 줍는다.\n",
            "문안에(거기도 문밖은 아니지만) 들어간답시는 앞집 마마님을 전찻길까지 모셔다 드린 것을 비롯으로 행여나 손님이 있을까 하고 정류장에서 어정어정하며 내리는 사람 하나하나에게 거의 비는 듯한 눈결을 보내고 있다가 마침내 교원인 듯한 양복쟁이를 동광학교(東光學校)까지 태워다 주기로 되었다.\n",
            "앓는 어미 곁에서 배고파 보채는 개똥이 (세살먹이)에게 죽을 사줄 수도 있다 - 팔십 전을 손에 쥔 김 첨지의 마음은 푼푼하였다.\n",
            "하고, 목메인 소리가 뒤를 따랐다.\n",
            "이때에 빽빽 소리가 응아 소리로 변하였다.\n",
            "구태여 쓰려면 못쓸 바도 아니로되 그는 병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다는 자기의 신조(信條)에 어디까지 충실하였다.\n",
            "이상하게도 꼬리를 맞물고 덤비는 이 행운 앞에 조금 겁이 났음이다.\n",
            "“여보게 치삼이, 내 우스운 이야기 하나 할까.\n",
            "하고 김첨지는 잠깐 주저하였다.\n",
            "운수가, 좋더니만…….\n",
            "하여간 김첨지는 방문을 왈칵 열었다.\n",
            "남편이 와도 일어나지를 못해.”\n",
            "노동으로 하여 흐른 땀이 식어지자 굶주린 창자에서, 물 흐르는 옷에서 어슬어슬 한기가 솟아나기 비롯하매 일 원 오십 전이란 돈이 얼마나 괜찮고 괴로운 것인 줄 절절히 느끼었다.\n",
            "혹은 김첨지도 이 불길한 침묵을 짐작했는지도 모른다.\n",
            "구역을 나게 하는 추기 ― 떨어진삿자리 밑에서 나온 먼지내 빨지 않은 기저귀에서 나는 똥내와 오줌내 가지각색 때가 켜켜이 앉은 옷내 병인의 땀 썩은 내가 섞인 추기가 무딘 김첨지의 코를 찔렀다.\n",
            "창경원 앞까지 다다라서야 그는 턱에 닿은 숨을 돌리고 걸음도 늦추잡았다.\n",
            "입이 붙었어, 이 오라질 년!”\n",
            "슬근슬근 가까이 가서 인력거 타시랍시요 하고 손가방을 받으랴니까 내 손을 탁 뿌리치고 홱 돌아서더니만 ‘왜 남을 이렇게 귀찮게 굴어!’ 그 소리야말로 꾀꼬리 소리지, 허허!”\n",
            "정거장까지 끌어다 주고 그 깜짝 놀란 일 원 오십 전을 정말 제 손에 쥠에 제 말마따나 십리나 되는 길을 비를 맞아 가며 질퍽거리고 온 생각은 아니하고 거저나 얻은 듯이 고마웠다.\n",
            "전차가 빡빡하게 사람을 싣고 움직이기 시작하였을 제 타고 남은 손 하나가 있었다.\n",
            "온몸이 옹송그려지며 당장 그 자리에 엎어져 못 일어날 것 같았다.\n",
            "김첨지는 대어섰다.\n",
            "다리를 재게 놀려야만 쉴새없이 자기의 머리에 떠오르는 모든 근심과 걱정을 잊을 듯이.\n",
            "이라고 고함을 친 게 수상하다.\n",
            "맞붙들고 앉았으면 누가 먹여 살릴 줄 알아.”\n",
            "“이 난장맞을 년, 남편이 들어오는데 나와 보지도 않아, 이 오라질 년.”\n",
            "“ 여보게 김첨지, 자네  문안 들어갔다 오는 모양일세그려.\n",
            "그런데 여보게, 자네 왼몸이 어째 물독에 빠진 새앙쥐 같은가.\n",
            "조밥도 굶기를 먹다시피 하는 형편이니 물론 약 한 첩 써본 일이 없다.\n",
            "“아따 이놈아, 사십 전이 그리 끔찍하냐.\n",
            "“일 원 오십 전은 너무 과한데.”\n",
            "바퀴도 어떻게 속히 도는지 구른다느니보다 마치 얼음을 지쳐 나가는 스케이트 모양으로 미끄러져 가는 듯하였다.\n",
            "“그래 남대문 정거장까지 얼마란 말이요?”\n",
            "정거장까지 가잔 말을 들은 순간에 경련적으로 떠는 손 유달리 큼직한 눈울 듯한 아내의 얼굴이 김첨지의 눈앞에 어른어른하였다.\n",
            "잇수로 치면 여기서 거기가 시오 리가 넘는답니다.\n",
            "한 걸음 두 걸음 집이 가까워 갈수록 그의 마음조차 괴상하게 누그러웠다.\n",
            "그의 우글우글 살찐 얼굴에 주홍이 덧는 듯, 온 턱과 뺨을 시커멓게 구레나룻이 덮였거늘 노르탱탱한 얼굴이 바짝 말라서 여기저기 고랑이 패고 수염도 있대야 턱밑에만 마치 솔잎 송이를 거꾸로 붙여 놓은 듯한 김첨지의 풍채하고는 기이한 대상을 짓고 있었다.\n",
            "라고, 모기 소리같이 중얼거리고 숨을 걸그렁걸그렁하였다.\n",
            "하고 학생은 초조한 듯이 인력거꾼의 얼굴을 바라보며 혼자말같이,\n",
            "“아따, 재미 안 좋다고 술 못 먹을 낸가.\n",
            "참오늘 운수가 좋았느니.”\n",
            "“왜 이래, 남 귀치않게.”\n",
            "이 육시를 할 돈!”\n",
            "발로 차도 그 보람이 없는 걸 보자 남편은 아내의 머리맡으로 달려들어 그야말로 까치집 같은 환자의 머리를 꺼들어 흔들며,\n",
            "그렇지 않으면왜 구두를 채 신지 못해서 질질 끌고, 비록 고구라 양복일망정 노박이로 비를 맞으며 김첨지를 뒤쫓아 나왔으랴.\n",
            "이 더러운 놈들아, 내가 돈이 없나, 다리뼉다구를 꺾어 놓을놈들 같으니.”\n",
            "“뭐, 마누라가 죽다니, 언제?”\n",
            "운대도 온 얼굴을 찡그려 붙여서 운다는 표정을 할 뿐이다.\n",
            "“자네는 벌써 한잔한 모양일세그려.\n",
            "그는 두리번두리번 사면을 살피었다.\n",
            "“그래 얼마를 벌었단 말인가.”\n",
            "김첨지는 원망스럽게 전차 타는 이를 노리고 있었다.\n",
            "그럴 즈음에 마침 길가 선술집에서 그의 친구 치삼이가 나온다.\n",
            "“이런 오라질 년!\n",
            "그러나 그 웃음 소리들이 사라도 지기 전에 김첨지는 훌쩍훌쩍 울기 시작하였다.\n",
            "“젠장맞을 것, 이 비를 맞으며 빈 인력거를 털털거리고 돌아를 간담.\n",
            "딸국딸국 하고 숨 모으는 소리도 나는 듯싶다.\n",
            "김첨지의 눈은 벌써 개개 풀리기 시작하였다.\n",
            "그웃음 소리가 어떻게 컸던지 술집에 있는 이의 눈은 모두 김첨지에게로 몰리었다.\n",
            "한동안 값으로 승강이를 하다가 육십 전에 인사동까지 태워다 주기로 하였다.\n",
            "아마도 그 학교 기숙사에 있는 이로 동기방학을 이용하여 귀향하려 함이리라.\n",
            "석쇠에 얹힌 떡 두 개를 숭덩숭덩 썰어서 볼을 불룩거리며 또 곱배기 두 잔을 부어라 하였다.\n",
            "따라서 의사에게 보인 적이 없으니 무슨 병인지는 알 수 없으되 반듯이 누워 가지고 일어나기는 새로 모로도 못 눕는 걸 보면 중증은 중증인 듯.\n",
            "김첨지는 연해 코를 들이마시며,\n",
            "“우리 마누라가 죽었다네.”\n",
            "그 학생을 태우고 나선 김첨지의 다리는 이상하게 거뿐하였다.\n",
            "새삼스러운 염려가 그의 가슴을 눌렀다.\n",
            "그 목소리는 몸집과 딴판으로 연하고 싹싹하였다.\n",
            "왜 눈을 바루 뜨지 못해!”하고 앓는 이의 뺨을 한 번 후려갈겼다.\n",
            "“이놈아, 그걸 먹고 취할 내냐, 어서 더 먹어.”\n",
            "하자마자 허리춤을 훔칫훔칫하더니 일 원짜리 한 장을 꺼내어 중대가리 앞에 펄쩍 집어던졌다.\n",
            "이날이야말로 동소문 안에서 인력거꾼 노릇을 하는 김첨지에게는 오래간만에도 닥친 운수 좋은 날이었다 .\n",
            "그르렁거리는 숨소리조차 들을 수 없다.\n",
            "더구나 이날 이때에 이 팔십 전이라는 돈이 그에게 얼마나 유용한지 몰랐다.\n",
            "무슨 일이 있더라도 제일 제이의 행운을 곱친 것보다고 오히려 갑절이 많은 이 행운을 놓칠 수 없다 하였다.\n",
            "오늘 손을 태고 정거장에 가지 않았겠나.”\n",
            "“……”\n",
            "그러자 엉엉 하고 우는 개똥이의 곡성을 들은 듯싶다.\n",
            "자기를 살려 준 은인이나 무엇같이 고맙기도 하였다.\n",
            "에 가까운 벌이를 하였다는 기쁨을 할 수 있으면 오래 지니고 싶었다.\n",
            "“이년아, 말을 해, 말을!\n",
            "정말 죽었나 버이.”\n",
            "그리고 술을 붓는 열다섯 살 됨직한 중대가리에게로 달려들며,\n",
            "마음은 급하고 불길은 달지 않아 채 익지도 않은 것을 그 오라질년이 숟가락은 고만두고 손으로 움켜서 두 뺨에 주먹덩이 같은 혹이 불거지도록 누가 빼앗을듯이 처박질하더니만 그날 저녁부터 가슴이 땡긴다, 배가 켕긴다고 눈을 흡뜨고 지랄병을 하였다.\n",
            "얼마 만에 기차는 왔고 수십 명이나 되는 손이 정류장으로 쏟아져 나왔다.\n",
            "벽에 맞아 떨어진 돈은 다시 술 끓이는 양푼에 떨어지며 정당한 매를 맞는다는 듯이 쨍 하고 울었다.\n",
            "“안 죽었어, 안 죽었대도 그래.”\n",
            "하고는 치삼의 귀를 잡아 치며 취한 이는 부르짖었다.\n",
            "“빌어먹을 깍쟁이 같은 년, 누가 저를 어쩌나, ‘왜 남을 귀찮게 굴어!’ 어이구 소리가 처신도 없지, 허허.”\n",
            "이 고함이야말로 제 몸을 엄습해 오는 무시무시한 증을 쫓아 버리려는 허장성세인 까닭이다.\n",
            "“이년아, 죽었단 말이냐, 왜 말이 없어.”\n",
            "김첨지는 화증을 내며 확신 있게 소리를 질렀으되 그 소리엔 안 죽은 것을믿으려고 애쓰는 가락이 있었다.\n",
            "이러다가 누운 이의 흰 창을 덮은 위로 치뜬 눈을 알아보자마자,\n",
            "돈 많이 벌었을테니 한잔 빨리게.”\n",
            "하고 치삼이도 어느 불안을 느끼는 듯이 김첨지에게 또 돌아가라고 권하였다.\n",
            "만일 청각(聽覺)이 예민한 이 같으면 그 빡빡소리는 빨 따름이요, 꿀떡꿀떡 하고 젖 넘어가는 소리가 없으니 빈 젖을 빤다는 것도 짐작할는지 모르리라.\n",
            "모든 사람은 일시에 웃었다.\n",
            "하는 말 끝엔 목이 메였다.\n",
            "내가 이렇게 아픈데…….”\n",
            "집이 차차 멀어 갈수록 김첨지의 걸음에는 다시금 신이 나기 시작하였다.\n",
            "오늘 운수가 괴상하게도 좋으니까 그런 요행이 또 한번 없으리라고 누가 보증하랴.\n",
            "달음질을 한다느니보다 거의 나는 듯하였다.\n",
            "김첨지는 어랍시요 하고 물러섰다.\n",
            "선술집은 훈훈하고 뜨뜻하였다.\n",
            "굉장하게 큰 가방을 들고 있는걸 보면 아마 붐비는 차 안에 짐이 크다 하여 차장에게 밀려 내려온 눈치였다.\n",
            "“예, 예.”\n",
            "치삼은 흥이 조금 깨어지는 얼굴로,\n",
            "그렇다고 정거장 인력거꾼의 등쌀이 무서우니 정거장 앞에 섰을 수는 없었다.\n",
            "이런 젠장맞을 술을 왜 안 부어…… 괜찮다 괜찮다, 막 먹어도 상관이 없어.\n",
            "그때 김첨지는 열화와 같이 성을 내며,\n",
            "이 말이 저도 모를 사이에 불쑥 김첨지의 입에서 떨어졌다.\n",
            "“왜 이리우, 기차 놓치겠구먼.”\n",
            "궂은비는 의연히 추적추적 내린다.\n",
            "주정꾼이 이 눈치를 알아보고 화를 버럭 내며,\n",
            "하고 탄 이의 초조한 부르짖음이 간신히 그의 귀에 들어왔다.\n",
            "이 환자가 그러고도 먹는 데는 물리지 않았다.\n",
            "“인천 차가 열한 점에 있고 그 다음에는 새로 두 점이든가.”\n",
            "김첨지는 구걸하는 거지나 무엇같이 연해연방 그의 기색을 살피며,\n",
            "“으응, 이것 봐, 아무 말이 없네.”\n",
            "라고 야단을 쳤다.\n",
            "병이 이대도록 심해지기는 열흘전에 조밥을 먹고 체한 때문이다.\n",
            "치삼은 의아한 듯이 김첨지를 보며,\n",
            "“여보게 또 붓다니, 벌써 우리가 넉 잔씩 먹었네, 돈이 사십 전일세.”\n",
            "제 자식뻘밖에 안 되는 어린 손님에게 몇 번 허리를 굽히며,\n",
            "그러나 그의 예감(豫感)은 틀리지 않았다.\n",
            "그 여학생인지 만지가 한참은 매우 때깔을 빼며 입술을 꼭 다문 채 김첨지를 거들떠보지도 않았다.\n",
            "다만 이 무덤 같은 침묵을 깨뜨리는 ― 깨뜨린다느니보다 한층 더 침묵을 깊게 하고 불길하게 하는 빡빡 하는 그윽한 소리, 어린애의 젖 빠는 소리가 날 뿐이다.\n",
            "그야말로 재수가 옴붙어서 근 열흘 동안 돈 구경도 못한 김첨지는 십 전짜리 백동화 서 푼, 또는 다섯 푼이 찰깍 하고 손바닥에 떨어질 제 거의 눈물을 흘릴 만큼 기뻤었다.\n",
            "이윽고 끄는 이의 다리는 무거워졌다.\n",
            "졸부나 된 듯이 기뻤다.\n",
            "이런 빌어먹을 제 할미를 붙을 비가 왜 남의 상판을 딱딱 때려!”\n",
            "그 오라질 년이 밥을 죽이지.\n",
            "라고 주의시켰다.\n",
            "“에이, 오라질년, 조랑복은 할 수가 없어, 못 먹어 병, 먹어서 병!\n",
            "개똥이가 물었던 젖을 빼어 놓고 운다.\n",
            "집이라 해도 물론셋집이요 또 집 전체를 세든 게 아니라 안과 뚝 떨어진 행랑방 한 간을 빌려 든 것인데 물을 길어 대고 한 달에 일 원씩 내는 터이다.\n",
            "문득 김첨지는 미친 듯이 제 얼굴을 죽은 이의 얼굴에 한데 비비대며 중얼거렸다.\n",
            "왜 나를 바라보지 못하고 천장만 보느냐, 응.”\n",
            "하고 우는 이의 팔을 잡아당기었다.\n",
            "“남대문 정거장까지 얼마요.”\n",
            "인제 나한테 속았다.”\n",
            "김첨지는 이 친구를 만난 게 어떻게 반가운지 몰랐다.\n",
            "김첨지는 취중에도 설렁탕을 사가지고 집에 다다랐다.\n",
            "하고, 김첨지는 또다시 달음질하였다.\n",
            "“그래서.”\n",
            "“어, 이 사람 취했군, 그만두세.”\n",
            "그러나 빈 인력거를 털털거리며 이 우중에 돌아갈 일이 꿈밖이었다.\n",
            "“에미를 붙을 이 오라질 놈들 같으니, 이놈 내가 돈이 없을 줄 알고.”\n",
            "어쩌란 말이야!\n",
            "“아씨, 인력거 아니 타시랍시요.”\n",
            "자기를 불러 멈춘 사람이 그 학교 학생인 줄 김첨지는 한번 보고 짐작할 수 있었다.\n",
            "“일 원 오십 전만 줍시요.”\n",
            "꼬리를 굴리는 행운이 꼭 자기를 기다리고 있다고 내기를 해도 좋을 만한 믿음을 얻게 되었다.\n",
            "그리고 병자의 움쑥 들어간 눈이 원망하는 듯이 자기를 노리는 듯하였다.\n",
            "땀과 빗물이 섞여 흐르는 목덜미를 기름주머니가 다된 왜목 수건으로 닦으며, 그 학교 문을 돌아 나올 때였다.\n",
            "“남대문 정거장까지 말씀입니까.”\n",
            "이런 말을 하며 학생은 고개를 기웃하였다.\n",
            "인력거가 무거워지매 그의 몸은 이상하게도 가벼워졌고 그리고 또 인력거가 가벼워지니 몸은 다시금 무거워졌건만 이번에는 마음조차 초조해 온다.\n",
            "조밥도 못 먹는 년이 설렁탕은.\n",
            "눌러 곱배기 한 잔을 또 마셨다.\n",
            "라고 중얼거린다.\n",
            "“봐라 봐!\n",
            "오늘 내가 돈을 막 벌었어.\n",
            "웃는 이는 더욱 웃으며,\n",
            "첫 번에 삼십전 , 둘째 번에 오십전 - 아침 댓바람에 그리 흉치 않은 일이었다.\n",
            "그러나 발길에 채이는 건 사람의 살이 아니고 나무등걸과 같은 느낌이 있었다.\n",
            "김첨지의 눈시울도 뜨끈뜨끈하였다.\n",
            "추어탕을 끓이는 솥뚜껑을 열 적마다 뭉게뭉게 떠오르는 흰김 석쇠에서 뻐지짓뻐지짓 구워지는 너비아니구이며 제육이며 간이며 콩팥이며 북어며 빈대떡……이 너저분하게 늘어놓인 안주 탁자에 김첨지는 갑자기 속이 쓰려서 견딜 수 없었다.\n",
            "정거장을 떠나는 그의 발길은 힘 하나 없었다.\n",
            "그는 슬근슬근 그 여자의 곁으로 다가들었다.\n",
            "하고 빙글빙글 웃는 차부의 얼굴에는 숨길 수 없는 기쁨이 넘쳐흘렀다.\n",
            "“그러면 달라는 대로 줄 터이니 빨리 가요.”\n",
            "나무 등걸이나 무엇 같고 제 것 같지도 않은 다리를 연해 꾸짖으며 질팡갈팡 뛰는 수밖에 없었다.\n",
            "언뜻 깨달으니 김첨지는 인력거를 쥔 채 길 한복판에 엉거주춤 멈춰 있지 않은가.\n",
            "하고 김첨지는 얼굴을 펴서 웃었다.\n",
            "처음 것 둘째 것으로 고만 만족하였음일까?\n",
            "그러자 산 사람의 눈에서 떨어진 닭의 똥 같은 눈물이 죽은 이의 뻣뻣한 얼굴을 어룽어룽 적시었다.\n",
            "뚱뚱보는 말라깽이를 보던 맡에 부르짖었다.\n",
            "“오늘은 나가지 말아요.\n",
            "그리고 집을 나올 제 아내의 부탁이 마음이 켕기었다 - 앞집 마마님한테서 부르러 왔을 제 병인은 뼈만 남은 얼굴에 유일의 샘물 같은 유달리 크고 움푹한 눈에 애걸하는 빛을 띄우며,\n",
            "“안녕히 다녀옵시요.”\n",
            "집의 광경이 자꾸 눈앞에 어른거리어 인제 요행을 바랄 여유도 없었다.\n",
            "“이 사람이 정말 미쳤단 말인가.\n",
            "아니다 결코 아니다.\n",
            "“아씨, 정거장 애들보담 아주 싸게 모셔다 드리겠습니다.\n",
            "그의 아내가 기침으로 쿨룩거리기는 벌써 달포가 넘었다.\n",
            "“이런 오라질 년, 주야장천 누워만 있으면 제일이야.\n",
            "방 안에 들어서며 설렁탕을 한구석에 놓을 사이도 없이 주정꾼은 목청을 있는 대로 다 내어 호통을 쳤다.\n",
            "그럴 즈음에 그의 머리엔 또 새로운 광명이 비쳤나니 그것은 ‘이러구 갈 게 아니라 이 근처를 빙빙 돌며 차 오기를 기다리면 또 손님을 태우게 될는지도 몰라’란 생각이었다.\n",
            "또 한 잔 먹고 나서 김첨지는 치삼의 어깨를 치며 문득 껄껄 웃는다.\n",
            "“이 눈깔!\n",
            "소리를 벽력같이 지르고는 돌아선다.\n",
            "“인력거를 타시랍시요.”\n",
            "흐리고 비 오는 하늘은 어둠침침하게 벌써 황혼에 가까운 듯하다.\n",
            "“이놈, 오라질 놈, 왜 술을 붓지 않어.”\n",
            "라고 깍듯이 재우쳤다.\n",
            "순식간에 두부와 미꾸리 든 국 한 그릇을 그냥 물같이 들이켜고 말았다.\n",
            "김첨지는 입술과 수염에 붙은 술을 빨아들이고 나서 매우 만족한 듯이 그 솔잎 송이 수염을 쓰다듬으며,\n",
            "라고 외쳤다.\n",
            "치삼의 끄는 손을 뿌리치더니 김첨지는 눈물이 글썽글썽한 눈으로 싱그레웃는다.\n",
            "저놈의 인력거꾼이 저렇게 술이 취해 가지고 이 진땅에 어찌 가노, 라고 길 가는 사람이 걱정을 하리만큼 그의 걸음은 황급하였다.\n",
            "“나가지 말라도 그래, 그러면 일찍이 들어와요.”\n",
            "하고 김첨지는 엉엉 소리를 내어 운다.\n",
            "자네도 오늘 재미가 좋아 보이.”\n",
            "그렇지 않으면 대문에 들어서자마자 전에 없이,\n",
            "제 입으로 부르고도 스스로 그 엄청난 돈 액수에 놀랐다.\n",
            "댁이 어디신가요.”\n",
            "주린 창자는 음식맛을 보더니 더욱더욱 비어지며 자꾸자꾸 들이라 들이라 하였다.\n",
            "그 모양은 마치 자기 집 ― 곧 불행을 향하고 달아가는 제 다리를 제 힘으로는 도저히 어찌할 수 없으니 누구든지 나를 좀 잡아 다고, 구해 다고 하는 듯하였다.\n",
            "“금방 웃고 지랄을 하더니 우는 건 또 무슨 일인가.”\n",
            "“원 이 사람이, 참말을 하나 거짓말을 하나.\n",
            "웃음 소리들은 높아졌다.\n",
            "그는 몹시 화증을 내며 누구에게 반항이나 하는 듯이 게걸거렸다.\n",
            "김첨지는 취한 중에도 돈의 거처를 살피는 듯이 눈을 크게 떠서 땅을 내려다보다가 불시에 제 하는 짓이 너무 더럽다는 듯이 고개를 소스라치자 더욱 성을 내며,\n",
            "언 땅에 비가 내려 미끄럽기도 하였지만.\n",
            "곱배기 두 잔은 또 부어질 겨를도 없이 말려 가고 말았다.\n",
            "셋째 그릇을 받아 들었을 제 데우던 막걸리 곱배기 두 잔이 더웠다.\n",
            "“엣기 미친놈, 거짓말 말아.”\n",
            "“죽기는 누가 죽어.”\n",
            "“아따, 젠장맞을 년, 별 빌어먹을 소리를 다 하네.\n",
            "울다가 울다가 목도 잠겼고 또 울 기운조차 시진한 것 같다.\n",
            "치삼이와 같이 마시자 원원이 비었던 속이라 찌르를 하고 창자에 퍼지며 얼굴이 화끈하였다.\n",
            "“여보게 돈 떨어졌네, 왜 돈을 막 끼얹나.”\n",
            "그는 불행에 다닥치기 전 시간을 얼마쯤이라도 늘이려고 버르적거렸다.\n",
            "그 학생은 다짜고짜로,\n",
            "“거짓말은 왜, 참말로 죽었어, 참말로…… 마누라 시체를 집에 뻐들쳐 놓고 내가 술을 먹다니, 내가 죽일 놈이야, 죽일 놈이야.”\n",
            "오늘 돈 산더미같이 벌었는데.”\n",
            "컬컬한 목에 모주 한 잔도 적실 수 있거니와 그보다도 앓는 아내에게 설렁탕 한 그릇도 사다 줄 수 있음이다.\n",
            "치삼은 어이없이 주정뱅이를 바라보며,\n",
            "하고 훌쩍 뛰어나오려니까 환자는 붙잡을 듯이 팔을 내저으며,\n",
            "어서 이리 들어와 말리게.”\n",
            "이 눈깔!\n",
            "뒤에서 “인력거!” 하고 부르는 소리가 난다.\n",
            "“갔다가 그저 오기가 안됐데그려.\n",
            "하고 득의가 양양.\n",
            "하고 치삼의 주워 주는 돈을 받아,\n",
            "라는 소리와 함께 발길로 누운 이의 다리를 몹시 찼다.\n",
            "인제 설렁탕을 사줄 수도 있다.\n",
            "“아니올시다.\n",
            "그러면 집으로 가세, 가.”\n",
            "또 이런 진날은 좀 더 주셔야지요.”\n",
            "“오늘은 나가지 말아요, 내가 이렇게 아픈데” 이런 말이 잉잉 그의 귀에 울렸다.\n",
            "전차는 왔다.\n",
            "자기 집 가까이 다다른 까닭이다.\n",
            "그러나 그의 행운은 그걸로 그치지 않았다.\n",
            "오늘 가기로 작정은 하였건만 비는 오고, 짐은 있고 해서 어찌할 줄 모르다가 마침 김첨지를 보고 뛰어나왔음이리라.\n",
            "“삼십 원을 벌었어, 삼십 원을!\n",
            "“이 원수엣돈!\n",
            "라고 물었다.\n",
            "“또 부어, 또 부어.”\n",
            "그래 전차 정류장에서 어름어름하며 손님 하나를 태울 궁리를 하지 않았나.\n",
            "“으응, 또 대답이 없네.\n",
            "사흘 전부터 설렁탕 국물이 마시고 싶다고 남편을 졸랐다.\n",
            "관대한 어린 손님은 이런 말을 남기고 총총히 옷도 입고 짐도 챙기러 갈 데로 갔다.\n",
            "또 처먹고 지랄병을 하게.” 라고, 야단을 쳐보았건만, 못 사주는 마음이 시원치는 않았다.\n",
            "그 사품에 몇 푼 은전이 잘그랑 하며 떨어진다.\n",
            "그런데 이 누그러움은 안심에서 오는 게 아니요 자기를 덮친 무서운 불행을 빈틈없이 알게 될 때가 박두한 것을 두리는 마음에서 오는 것이다.\n",
            "“죽기는 왜 죽어, 생때같이 살아만 있단다.\n",
            "중대가리는 희희 웃고 치삼을 보며 문의하는 듯이 눈짓을 하였다.\n",
            "나도 아주먼네가 앓는단 말은 들었는데.”\n",
            "설마 오늘 내로 어떠랴 싶었다.\n",
            "쿨룩거리는 기침 소리도 들을 수 없다.\n",
            "그 중에서 손님을 물색하는 김첨지의 눈엔 양머리에 뒤축 높은 구두를 신고 망토까지 두른 기생 퇴물인 듯 난봉 여학생인 듯한 여편네의 모양이 띄었다.\n",
            "새침하게 흐린 품이 눈이 올 듯하더니 눈은 아니 오고 얼다가 만 비가 추적추적 내리는 날이었다.\n",
            "“이놈아 언제는, 오늘이지.”\n",
            "그때도 김첨지가 오래간만에 돈을 얻어서 좁쌀 한 되와 십 전짜리 나무 한 단을 사다 주었더니 김첨지의 말에 의지하면 그 오라질 년이 천방지축으로 냄비에 대고 끓였다.\n",
            "그는 이 우중에 우장도 없이 그 먼 곳을 철벅거리고 가기가 싫었음일까?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok-yxSOCCnri"
      },
      "source": [
        "토크나이저로 텍스트를 토큰화한 후 입력데이터와 출력 데이터로 구성.\n",
        "한 문장으로 이루어진 데이터는 문장 시작, 끝에 스페셜 토큰 할당\n",
        "\n",
        "입력데이터 tokens[:-1]로 맨 앞에서 맨 뒤 직전 토큰까지만 활용\n",
        "\n",
        "정답데이터를 tokens[1:]로 맨 앞 다음 토큰에서 맨 뒤 토큰까지 활용\n",
        "\n",
        "pad_sequences 함수를 통해 데이터 패딩 && np.array로 구성하여 학습데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic1JdTX3pOnn"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-qFv9v6vKk"
      },
      "source": [
        "**손실함수와 정확도 측정 함수**\n",
        "\n",
        "*loss_object*: 크로스 엔트로피로 손실 값을 측정하기 위한 객체\n",
        "\n",
        "*train_accuracy*: 정확도 측정을 위한 객체\n",
        "\n",
        "*loss_function:* 인자로 정답과 예측한 값을 받아서 두 개의 값을 비교해서 손실을 계산하며, real 값 중 0인 값 <PAD<PAD>>는 손실계산에서 뺌\n",
        "train_accuracy: 정확도를 체크\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "```\n",
        "정답 real에 포함되는 값 중 vocab[vocab.padding__token]인 것은 <<PAD>PAD>를 의미하는 값. 해당 값들은 True(1)가 되고 이를 제외한 나머지 값들은 False(0)\n",
        "\n",
        "치환된 요소들에 logical_not 함수를 적용하면 0->1,1->0으로 바뀜\n",
        "\n",
        "변경된 값을 loss_*=mask에 요소 간에 곱을 해주면 <<PAD>PAD>값은 loss_계산에서 빠짐\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbemuEkipQon"
      },
      "source": [
        "gpt_model.compile(loss=loss_function,\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=[accuracy_function])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZixsBU3UHnaG"
      },
      "source": [
        "gpt_model.compile을 통해 loss나 optimizer, metrics 등을 설정해서 미세조정 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZd_8Fk9pSnP",
        "outputId": "bd0ee329-7d68-4867-dcbb-295652dff893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = gpt_model.fit(input_data, output_data, \n",
        "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 5s 288ms/step - loss: 3.0125 - accuracy_function: 0.0982 - val_loss: 2.4778 - val_accuracy_function: 0.1146\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 2s 153ms/step - loss: 2.5107 - accuracy_function: 0.1250 - val_loss: 2.3959 - val_accuracy_function: 0.1319\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 2s 155ms/step - loss: 2.2720 - accuracy_function: 0.1398 - val_loss: 2.3877 - val_accuracy_function: 0.1452\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 2s 155ms/step - loss: 2.0568 - accuracy_function: 0.1531 - val_loss: 2.3767 - val_accuracy_function: 0.1586\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 2s 155ms/step - loss: 1.8500 - accuracy_function: 0.1653 - val_loss: 2.4113 - val_accuracy_function: 0.1710\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 2s 156ms/step - loss: 1.6485 - accuracy_function: 0.1765 - val_loss: 2.4904 - val_accuracy_function: 0.1823\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 2s 156ms/step - loss: 1.4492 - accuracy_function: 0.1888 - val_loss: 2.5912 - val_accuracy_function: 0.1943\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 2s 156ms/step - loss: 1.2511 - accuracy_function: 0.2010 - val_loss: 2.7008 - val_accuracy_function: 0.2065\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 3s 157ms/step - loss: 1.0544 - accuracy_function: 0.2136 - val_loss: 2.8500 - val_accuracy_function: 0.2189\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 3s 157ms/step - loss: 0.8935 - accuracy_function: 0.2264 - val_loss: 2.9818 - val_accuracy_function: 0.2314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr2li2BvH42J"
      },
      "source": [
        "에폭수와 학습 정확도는 비례 but 텍스트 생성과는 반비례"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY9BlKYvpUoR"
      },
      "source": [
        "DATA_OUT_PATH = './data_out'\n",
        "model_name = \"tf2_gpt2_finetuned_model\"\n",
        "\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "gpt_model.gpt2.save_pretrained(save_path)\n",
        "\n",
        "loaded_gpt_model = GPT2Model(save_path)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTM4BfqXtqL-"
      },
      "source": [
        "### **테스트-미세조정 후**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKWok90tc4q",
        "outputId": "81ab2c99-a1b8-47aa-eaa4-2d6f6dde49f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, greedy=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때에 마침 길가 선술집에서 김첨지는 그 여학생에게 반지를 내밀었다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrNTEJagtikj",
        "outputId": "64180e7b-dff7-4489-d181-835ac8480c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때 김첨지는 정신이 번쩍 든 듯이 눈을 동그랗게 뜨고 형주를 바라보았다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}