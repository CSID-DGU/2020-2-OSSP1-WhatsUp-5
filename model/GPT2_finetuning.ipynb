{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSID-DGU/2020-2-OSSP1-WhatsUp-5/blob/master/model/GPT2_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_jSucTag2t",
        "outputId": "f2017670-b4b0-4ff7-f708-7e773ff28e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!git clone https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoGPT2'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 100 (delta 52), reused 56 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (100/100), 548.66 KiB | 30.48 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PykXR-nTcSmh",
        "outputId": "d6b36bb0-e145-4531-dc0f-3c2afc593609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'='  '=0.1.85'   \u001b[0m\u001b[01;34mdrive\u001b[0m/   \u001b[01;34mKoGPT2\u001b[0m/   \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ra1de4JcX1e",
        "outputId": "18e0f84d-be34-4bdd-f9a0-03c1095a61c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd KoGPT2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgdJZxP8ytvv",
        "outputId": "17f1ac92-7353-4adf-ca81-803fe047b48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mimgs\u001b[0m/  \u001b[01;34mkogpt2\u001b[0m/  LICENSE  README.md  requirements.txt  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWPvIeGgcYv0",
        "outputId": "44605788-1e27-4134-8cab-b1c01195353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install ."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: mxnet==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.85 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.1.94)\n",
            "Collecting torch==1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0MB 21kB/s \n",
            "\u001b[?25hCollecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (0.16.0)\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.5.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, tokenizers, transformers\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: tokenizers 0.0.11\n",
            "    Uninstalling tokenizers-0.0.11:\n",
            "      Successfully uninstalled tokenizers-0.0.11\n",
            "  Found existing installation: transformers 2.4.1\n",
            "    Uninstalling transformers-2.4.1:\n",
            "      Successfully uninstalled transformers-2.4.1\n",
            "Successfully installed tokenizers-0.7.0 torch-1.5.0 transformers-2.11.0\n",
            "Processing /content/KoGPT2\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14054 sha256=21bd7fab9c48bdddef9e0019f098ec93eeb5071ff47d757f6fd6951a7f5c0ddc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z56v46c_/wheels/ac/e5/83/e839e6a987261c05b2e32cbd9770007e19f8ea7e2f2f7b9d3c\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcKQlRIufDt3"
      },
      "source": [
        "# 허깅페이스의 transformers 라이브러리와 SKT KoGPT2 모델 기반으로 작성된 코드\n",
        "---환경 설정 때문에 위에서 skt꺼 import 함\n",
        "--추후에는 해당 git에 있는 requirements.txt만 임포트 하면 될 것이라고 생각함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjx93wf8capL"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWc_hl3-e6m8"
      },
      "source": [
        "\n",
        "모델을 학습하기 위해 필요한 3까지 모듈\n",
        "1. TFGPT2LMHeadModel: 문장 생성\n",
        "2. gluonnlp의 SentencepieceTokenizer\n",
        "3. nlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-brv0bReLyZ"
      },
      "source": [
        "\n",
        "class GPT2Model(tf.keras.Model):\n",
        "    def __init__(self, dir_path):\n",
        "        super(GPT2Model, self).__init__()\n",
        "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        return self.gpt2(inputs)[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I-lC513ryzi"
      },
      "source": [
        "__init__ 함수에서 TFGPT2LMHeadModel을 생성해서 실행할 수 있게 구현함.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# self.gpt2(inputs)[0]\n",
        "```\n",
        "\n",
        "생성모델을 활용하기 위해서는 vocabulary에 대한 logit 값만 활용하도록 첫 번째 값인 last_hidden_states 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwCL_xYUgISb",
        "outputId": "0e8f7b4a-532b-4890-804c-24ed8f8f9d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "!unzip -o gpt_ckpt.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-26 12:07:04--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n",
            "--2020-10-26 12:07:04--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com/cd/0/inline/BB8u70UYAkV8XdaNILU0ZLT7TI9-keDOCvGirjZg8W-y9UgGkXZz_VGN-0MwFbHJXXj2crmAPxG7jPuRov6Gn4K_A8uWDzgCzIFVCaaTUvTEhxO80rCcQO0mdeaGSbhss6c/file# [following]\n",
            "--2020-10-26 12:07:05--  https://uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com/cd/0/inline/BB8u70UYAkV8XdaNILU0ZLT7TI9-keDOCvGirjZg8W-y9UgGkXZz_VGN-0MwFbHJXXj2crmAPxG7jPuRov6Gn4K_A8uWDzgCzIFVCaaTUvTEhxO80rCcQO0mdeaGSbhss6c/file\n",
            "Resolving uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com (uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601f:15::a27d:90f\n",
            "Connecting to uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com (uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BB9_hEXGx7w1fr3i_2TneSXUeFKNdyZf9xbSqwtgEpNnnDSbJnMen79ZDKc33tpSgnBIJ97pSJVUM1lx-59rArQ1H70E67cFbhJ2SPM193_UUHFtoNK3GkQcuPuDswf_43vs4S0rT2R6MUwc3xSoZaLhBAIcS6PYnaw-IxW2yxEqDx4RhjqkMWHR--vL430zEl7FvpJn7KHrxF8TQa3Iw1lqJRxyGYr1enMMisGnhes99jUXWRpwomvcvKEBE9bjJ5EenkVXwpkIvOmH_J0a5o5vmXuvXJR4W-XzYh7Kgo5jq0AqMImZg203iihrzbWx0Cb6KhPXIGvLXWfLXZjNoX3zrZ5FDg7EnFO_ylhttenu7Q/file [following]\n",
            "--2020-10-26 12:07:05--  https://uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com/cd/0/inline2/BB9_hEXGx7w1fr3i_2TneSXUeFKNdyZf9xbSqwtgEpNnnDSbJnMen79ZDKc33tpSgnBIJ97pSJVUM1lx-59rArQ1H70E67cFbhJ2SPM193_UUHFtoNK3GkQcuPuDswf_43vs4S0rT2R6MUwc3xSoZaLhBAIcS6PYnaw-IxW2yxEqDx4RhjqkMWHR--vL430zEl7FvpJn7KHrxF8TQa3Iw1lqJRxyGYr1enMMisGnhes99jUXWRpwomvcvKEBE9bjJ5EenkVXwpkIvOmH_J0a5o5vmXuvXJR4W-XzYh7Kgo5jq0AqMImZg203iihrzbWx0Cb6KhPXIGvLXWfLXZjNoX3zrZ5FDg7EnFO_ylhttenu7Q/file\n",
            "Reusing existing connection to uc4d96916498f3f5d3549bd12374.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460908853 (440M) [application/zip]\n",
            "Saving to: ‘gpt_ckpt.zip’\n",
            "\n",
            "gpt_ckpt.zip        100%[===================>] 439.56M  66.3MB/s    in 6.6s    \n",
            "\n",
            "2020-10-26 12:07:12 (67.1 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
            "\n",
            "Archive:  gpt_ckpt.zip\n",
            "   creating: gpt_ckpt/\n",
            "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
            "  inflating: gpt_ckpt/config.json    \n",
            "  inflating: gpt_ckpt/tf_model.h5    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtFR8N90R2BQ"
      },
      "source": [
        "**학습 파라미터 내려받고 준비하기**\n",
        "\n",
        "학습된 파라미터가 koGPT2의 경우 huggingface에 모델로 등록돼 있지 않아 파라미터를 다운로드 해야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMjhUSAGeXF6"
      },
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1GKb0rHSPLQ"
      },
      "source": [
        "모델 리소스 경로를 객체를 생성할 때 인자로 전달하여 학습된 파라미터를 가지는 GPT2 모델 객체 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hIfiaBAS5v-"
      },
      "source": [
        "## 사전 학습된 모델을 활용해 언어 생성 결과 확인\n",
        "\n",
        "단어 하나가 주어지면 문장을 만들어주는 방식으로"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llviuFQ6esS_"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "MAX_LEN = 30\n",
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token=None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk8bzPM4TDOQ"
      },
      "source": [
        "**토크나이저 생성**\n",
        "\n",
        "텍스트를 모델에 입력하려면 필요함. \n",
        "앞서 불러온 SentencepieceTokenizer와 nlp 모듈의 vocab 활용하여 단어 사전과 토크나이저 정의.\n",
        "\n",
        "\n",
        "> GPT2의 각 스페셜 토큰의 역할\n",
        "\n",
        "1.   <unk<unk>> 모르는 단어에 대한 토큰\n",
        "2.   <pad<pad>> 배치 데이터 길이 맞추는 용도\n",
        "3.   <s<s>> 문장의 시작을 알림\n",
        "4.   </s<s>> 문장의 종결을 알림\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUOBGwjDi-kU"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "    _logits = logits.numpy()\n",
        "    top_k = min(top_k, logits.shape[-1])  \n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "        _logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "        \n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    return tf.constant([_logits])\n",
        "\n",
        "\n",
        "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "    sent = seed_word\n",
        "    toked = tokenizer(sent)\n",
        "    \n",
        "    for _ in range(max_step):\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :]\n",
        "        if greedy:\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "        else:\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "        if gen == '</s>':\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnVpcWsgVXT1"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "```\n",
        "-see_word: 문장 생성의 시작 단어\n",
        "\n",
        "-model: 문장 생성을 수행할 모델\n",
        "\n",
        "-max_step: 생성 횟수를 제한\n",
        "\n",
        "-greedy: 모델 출력 결과에 대해 유연하게 문장 생성을 해줄 수 있는지 선택할 수 있도록\n",
        "\n",
        "*   greedy=true: 문장 출력 결과에 대해 가장 확률이 높은 단어만 선택\n",
        "*   greedy=false: 출력한 단어 가운데 확률 또는 순위가 높은 단어만 선택해 무작위 생성\n",
        "\n",
        "+) top_k와 top_p 파라미터:  false인 경우 사용. top_k는 확률이 높은 순새대로 k번째까지 필터링. top_p는 일정 확률값 이상인 단어에 대해 필터링\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#  sent = seed_word\n",
        "   toked = tokenizer(sent)\n",
        "```\n",
        "\n",
        "문장 시작 단어를 변수에 할당하고 토크나이즈\n",
        "\n",
        "\n",
        "```\n",
        "# for _ in range(max_step):\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :]\n",
        "```\n",
        "문장 생성을 할 수 있는 반복문. 토크나이즈된 단어를 인덱스로 변환하고 모델에 입력값으로 넣어 출력값을 받음. 모델의 출력값에 대해서는 문장에서 마지막 단어만 선택\n",
        "\n",
        "```\n",
        "# if gen == '</s>':\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "```\n",
        "생성된 텍스트 토큰이 문장의 끝을 알리는 </s</s>>토큰이면 생성 stop하고 앞서 만들어진 텍스트에 덧붙임\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8BB75iNtwEs"
      },
      "source": [
        "### **테스트-미세 조정 전**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfL6bgrnjFo_",
        "outputId": "a0d8f416-d6ca-46ad-f5ed-b4fceb076f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, greedy=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때부터                                                                                                   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX3PwhRzA7ur"
      },
      "source": [
        "greedy 방식: 확률이 가장 높은 단어만 선택, 학습한 바이어스에 따라 일관된 문장만 출력, 반복되는 단어가 출력되는 결과가 나올 수도 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT0k9ZDckGgs",
        "outputId": "16345111-ac2c-4c7b-eb85-f8f9f54e52e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때  어떤 생각을 품앗으신가요?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-ZNdM_lBvCQ"
      },
      "source": [
        "샘플링 방식: 좀 더 자연스러움"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpd17unbkuF_"
      },
      "source": [
        "# 소설 텍스트 데이터 전처리하기\n",
        "\n",
        "미세 조정할 학습 데이터 구성---학습할 데이터는 운수 좋은 날"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5oOt3j8kPu0",
        "outputId": "444b699b-e2be-4e81-ff0f-ddafdc5a2508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!git clone https://github.com/NLP-kr/tensorflow-ml-nlp-tf2.git"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-ml-nlp-tf2'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 1531 (delta 48), reused 14 (delta 4), pack-reused 1443\u001b[K\n",
            "Receiving objects: 100% (1531/1531), 200.94 MiB | 27.07 MiB/s, done.\n",
            "Resolving deltas: 100% (920/920), done.\n",
            "Checking out files: 100% (83/83), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLcx5Df2lbV_",
        "outputId": "a8146b52-adab-4c55-a113-b52351b9143c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgpt_ckpt\u001b[0m/     \u001b[01;34mimgs\u001b[0m/    LICENSE    requirements.txt  \u001b[01;34mtensorflow-ml-nlp-tf2\u001b[0m/\n",
            "gpt_ckpt.zip  \u001b[01;34mkogpt2\u001b[0m/  README.md  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PdLt4mHlzwX",
        "outputId": "cc12c595-fd98-4133-9b38-3616bd891101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd tensorflow-ml-nlp-tf2/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2/tensorflow-ml-nlp-tf2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00KpWfpYo8ty",
        "outputId": "671ca70c-9345-4c66-cac2-4f4c7412a6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd 7.PRETRAIN_METHOD"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/KoGPT2/tensorflow-ml-nlp-tf2/7.PRETRAIN_METHOD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0MN0S7qzwXc",
        "outputId": "a85c2173-8c59-49bb-c857-3bab2c016781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.2.1.bert_finetune_NSMC.ipynb     7.2.5.KorQuAD_EDA.ipynb\n",
            "7.2.2.bert_finetune_KorNLI.ipynb   7.4.1.gpt2_finetune_LM.ipynb\n",
            "7.2.2.KorNLI_EDA.ipynb             7.4.2.gpt2_finetune_NSMC.ipynb\n",
            "7.2.3.bert_finetune_NER.ipynb      7.4.3.gpt2_finetune_KorNLI.ipynb\n",
            "7.2.3.NER_EDA.ipynb                7.4.4.gpt2_finetune_KorSTS.ipynb\n",
            "7.2.4.bert_finetune_KorSTS.ipynb   \u001b[0m\u001b[01;34mdata_in\u001b[0m/\n",
            "7.2.4.KorSTS_EDA.ipynb             README.md\n",
            "7.2.5.bert_finetune_KorQuAD.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC8yDDVFpGRh"
      },
      "source": [
        "DATA_IN_PATH = './data_in/KOR/'\n",
        "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
        "\n",
        "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE).readlines()]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q41FEzHIB74E"
      },
      "source": [
        "학습데이터는 소설 텍스트를 먼저 문장별로 분리해둔 텍스트 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0DTlbHvpMuR"
      },
      "source": [
        "input_data = []\n",
        "output_data = []\n",
        "\n",
        "for s in sents:\n",
        "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
        "    input_data.append(tokens[:-1])\n",
        "    output_data.append(tokens[1:])\n",
        "\n",
        "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "\n",
        "input_data = np.array(input_data, dtype=np.int64)\n",
        "output_data = np.array(output_data, dtype=np.int64)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok-yxSOCCnri"
      },
      "source": [
        "토크나이저로 텍스트를 토큰화한 후 입력데이터와 출력 데이터로 구성.\n",
        "한 문장으로 이루어진 데이터는 문장 시작, 끝에 스페셜 토큰 할당\n",
        "\n",
        "입력데이터 tokens[:-1]로 맨 앞에서 맨 뒤 직전 토큰까지만 활용\n",
        "\n",
        "정답데이터를 tokens[1:]로 맨 앞 다음 토큰에서 맨 뒤 토큰까지 활용\n",
        "\n",
        "pad_sequences 함수를 통해 데이터 패딩 && np.array로 구성하여 학습데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic1JdTX3pOnn"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-qFv9v6vKk"
      },
      "source": [
        "**손실함수와 정확도 측정 함수**\n",
        "\n",
        "*loss_object*: 크로스 엔트로피로 손실 값을 측정하기 위한 객체\n",
        "\n",
        "*train_accuracy*: 정확도 측정을 위한 객체\n",
        "\n",
        "*loss_function:* 인자로 정답과 예측한 값을 받아서 두 개의 값을 비교해서 손실을 계산하며, real 값 중 0인 값 <PAD<PAD>>는 손실계산에서 뺌\n",
        "train_accuracy: 정확도를 체크\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "```\n",
        "정답 real에 포함되는 값 중 vocab[vocab.padding__token]인 것은 <<PAD>PAD>를 의미하는 값. 해당 값들은 True(1)가 되고 이를 제외한 나머지 값들은 False(0)\n",
        "\n",
        "치환된 요소들에 logical_not 함수를 적용하면 0->1,1->0으로 바뀜\n",
        "\n",
        "변경된 값을 loss_*=mask에 요소 간에 곱을 해주면 <<PAD>PAD>값은 loss_계산에서 빠짐\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbemuEkipQon"
      },
      "source": [
        "gpt_model.compile(loss=loss_function,\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=[accuracy_function])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZixsBU3UHnaG"
      },
      "source": [
        "gpt_model.compile을 통해 loss나 optimizer, metrics 등을 설정해서 미세조정 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZd_8Fk9pSnP",
        "outputId": "9fbf779b-5d12-4493-9e62-76788b89e602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "history = gpt_model.fit(input_data, output_data, \n",
        "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 0.9604 - accuracy_function: 0.4278 - val_loss: 2.6842 - val_accuracy_function: 0.4339\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 0.8306 - accuracy_function: 0.4423 - val_loss: 2.7775 - val_accuracy_function: 0.4481\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 0.7311 - accuracy_function: 0.4557 - val_loss: 2.8309 - val_accuracy_function: 0.4615\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 3s 171ms/step - loss: 0.6553 - accuracy_function: 0.4686 - val_loss: 2.8834 - val_accuracy_function: 0.4744\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 3s 173ms/step - loss: 0.5765 - accuracy_function: 0.4813 - val_loss: 2.9562 - val_accuracy_function: 0.4866\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 3s 174ms/step - loss: 0.5058 - accuracy_function: 0.4934 - val_loss: 3.0073 - val_accuracy_function: 0.4984\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 3s 175ms/step - loss: 0.4515 - accuracy_function: 0.5047 - val_loss: 3.0634 - val_accuracy_function: 0.5095\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.4148 - accuracy_function: 0.5155 - val_loss: 3.0719 - val_accuracy_function: 0.5197\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.3667 - accuracy_function: 0.5254 - val_loss: 3.1189 - val_accuracy_function: 0.5296\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 3s 179ms/step - loss: 0.3365 - accuracy_function: 0.5348 - val_loss: 3.1096 - val_accuracy_function: 0.5387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr2li2BvH42J"
      },
      "source": [
        "에폭수와 학습 정확도는 비례 but 텍스트 생성과는 반비례"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY9BlKYvpUoR"
      },
      "source": [
        "DATA_OUT_PATH = './data_out'\n",
        "model_name = \"tf2_gpt2_finetuned_model\"\n",
        "\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "gpt_model.gpt2.save_pretrained(save_path)\n",
        "\n",
        "loaded_gpt_model = GPT2Model(save_path)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTM4BfqXtqL-"
      },
      "source": [
        "### **테스트-미세조정 후**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKWok90tc4q",
        "outputId": "6ffe5835-ef59-4a28-8bf4-47da9686af03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, greedy=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때에                                                                                                   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrNTEJagtikj",
        "outputId": "d5c192f4-f3b7-42c1-e886-4da916571839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'이때 에                                                                                                  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiGlG06RnjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}