# -*- coding: utf-8 -*-
"""health_chosun_crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10YT9-DNKxldzuBLU4aelLFsJbs4b4hcY

# Data Crawling
"""


from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import UnexpectedAlertPresentException
from bs4 import BeautifulSoup
import time
import csv
import datetime
import pandas as pd


def get_article():
    n = int(input("keyword 검색(0), 일반 검색(1) : "))
    if n:
        print("일반 검색")
        return health_chosun_crawling()
    else:
        print("keyword 검색")
        return health_chosun_crawling_keyword()


def health_chosun_crawling_keyword():
    #공공보건 포털사이트의 뉴스 기사를 크롤링함
    site = 'http://health.chosun.com/list.html?menu=&more_menu=&more_smenu=&nowcode=1&type=&pn=1'

    #csv 파일 저장을 위한 f, utf-8을 인코딩으로 사용
    #f = open(path, 'w',encoding='utf-8')

    # 데이터 프레임 생성
    df = pd.DataFrame(columns=['name', 'date', 'content'])
    #파일 객체를 생성
    #wr = csv.writer(f)

    row = 1

    break_all = False
    #chrome으로 driver를 실행
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)

    #조건 성립 시간을 위한 wait 객체 설정
    wait = WebDriverWait(driver,20)

    #driver의 브라우저 크기를 최대화
    driver.maximize_window()
    #최대화를 하지 않았을 경우 마우스 포인터가 정확히 div를 클릭하지 못하여 크롤링을 못할 수 있음

    #웹의 컴포넌트 로딩을 기다리기 위한 implicitly_wait 함수, 로딩이 끝나면 바로 진행된다.
    driver.implicitly_wait(3)

    #미리 설정해둔 공공보건 포털사이트로 이동한다.
    driver.get(site)

    #크롤링할 기사의 날짜 범위 설정
    print("크롤링할 기사의 날짜 범위를 설정하세요")

    date_entry = input('시작 날짜(YYYY-MM-DD 형식) : ')
    year, month, day = map(int, date_entry.split('-'))
    date1 = datetime.date(year, month, day)

    date_entry = input('종료 날짜(YYYY-MM-DD 형식) : ')
    year, month, day = map(int, date_entry.split('-'))
    date2 = datetime.date(year, month, day)

    #기사의 제목, 내용, 생성날짜를 저장할 변수를 설정한다.
    name=""
    content=""
    date=""
    
    print("크롤링할 기사의 키워드를 설정하세요")
    keyword = input("기사의 키워드 : ")
    input_text = driver.find_element_by_id("query")
    input_text.send_keys(keyword)
    
    input_button = driver.find_element_by_class_name("searchBtn2016")
    input_button.send_keys(Keys.RETURN)

    medical_news = driver.find_elements_by_css_selector("div.btn_srch_more")
    for menu in medical_news:
        if menu.text.find("의료계뉴스 더보기") != -1:
            menu.find_element_by_css_selector("a").click()
            break;
    
    while True:
        time.sleep(1)
        idx = 1
        list_num = len(driver.find_elements_by_css_selector("div.cont ul.lst_srch_type01 li"))
        while True:
            #변수들의 초기화
            name=""
            content=""
            date=""
            while True:
                news = driver.find_element_by_css_selector("div.cont ul.lst_srch_type01 li:nth-child(" + str(idx) +") dl dt a")
                
                #잡다한 만화, 책광고 등등의 기사를 제외시킴
                if news.text.find("카툰") != -1 or news.text.find("해랑")!= -1 or news.text.find("서적") != -1 or news.text.find("튼튼선생") != -1 or news.text.find("양냥") != -1 or news.text.find("비만클리닉") != -1:
                    print(str(row) + "오류발생")
                    idx += 1
                    if(list_num < idx):
                        break;
                else:
                    break;      
            if(list_num < idx):
                break;
            news.click()
            try:
                alert = driver.switch_to.alert
                alert.accept()
                time.sleep(2)
            except :
                pass
            try :
                time.sleep(1)
                name = driver.find_element_by_css_selector("h2#title_text").text
                date = driver.find_element_by_css_selector("p#date_text").text.split(" ")[2]
                content = driver.find_element_by_css_selector("div#news_body_id").text

                year, month, day = map(int, date.split('.'))
                date3 = datetime.date(year, month, day)
                
                # 해당 기사의 날짜가 사용자 지정 범위 내인 경우에만 csv파일에 작성
                # 시작 날짜보다 이전에 작성된 기사라면 크롤링 종료
                if date1 > date3:
                    print("finish")
                    return df

                if date1 <= date3 and date3 <= date2:
                    #wr.writerow([row, name, date, content])
                    # append
                    df = df.append({'name': name, 'date': date, 'content': content}, ignore_index=True)
                    print(str(row) + " " + name + " / " + str(date3))

                row +=1
                driver.back()
                driver.implicitly_wait(5)
                idx += 1
                if(list_num < idx):
                    break;            
            except (AttributeError, NoSuchElementException, UnexpectedAlertPresentException):
                driver.back()
                idx += 1
                if(list_num < idx):
                    break;
                 
        page_now = int(driver.find_element_by_css_selector("div.pagenate_complex strong").text)
        pages = driver.find_elements_by_css_selector("div.pagenate_complex a")
        for page in pages:
            if page.text == str(page_now+1):
                page.click()
                break;
            if page.text == "다음":
                page.click()

def health_chosun_crawling():
    #공공보건 포털사이트의 뉴스 기사를 크롤링함
    site = 'http://health.chosun.com/list.html?menu=&more_menu=&more_smenu=&nowcode=1&type=&pn=1'

    #csv 파일 저장을 위한 f, utf-8을 인코딩으로 사용
    #f = open(path, 'w',encoding='utf-8')

    # 데이터 프레임 생성
    df = pd.DataFrame(columns=['name', 'date', 'content'])
    #파일 객체를 생성
    #wr = csv.writer(f)

    row = 1

    break_all = False
    #chrome으로 driver를 실행
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)

    #조건 성립 시간을 위한 wait 객체 설정
    wait = WebDriverWait(driver,20)

    #driver의 브라우저 크기를 최대화
    driver.maximize_window()
    #최대화를 하지 않았을 경우 마우스 포인터가 정확히 div를 클릭하지 못하여 크롤링을 못할 수 있음

    #웹의 컴포넌트 로딩을 기다리기 위한 implicitly_wait 함수, 로딩이 끝나면 바로 진행된다.
    driver.implicitly_wait(3)

    #미리 설정해둔 공공보건 포털사이트로 이동한다.
    driver.get(site)

    #크롤링할 기사의 날짜 범위 설정
    print("크롤링할 기사의 날짜 범위를 설정하세요")

    date_entry = input('시작 날짜(YYYY-MM-DD 형식) : ')
    year, month, day = map(int, date_entry.split('-'))
    date1 = datetime.date(year, month, day)

    date_entry = input('종료 날짜(YYYY-MM-DD 형식) : ')
    year, month, day = map(int, date_entry.split('-'))
    date2 = datetime.date(year, month, day)

    #기사의 제목, 내용, 생성날짜를 저장할 변수를 설정한다.
    name=""
    content=""
    date=""

    while True:
        time.sleep(1)
        idx = 1
        list_num = len(driver.find_elements_by_css_selector("dl.list_item"))
        while True:
            #변수들의 초기화
            name=""
            content=""
            date=""
            while True:
                news_list = driver.find_element_by_css_selector("div#list_area dl:nth-child(" + str(idx) +")")
                news = news_list.find_element_by_css_selector('dt a')
                news_text = news_list.text
                #잡다한 만화, 책광고 등등의 기사를 제외시킴
                if news.text.find("카툰") != -1 or news.text.find("해랑")!= -1 or news.text.find("서적") != -1 or news.text.find("튼튼선생") != -1 or news.text.find("양냥") != -1 or news_text.find("비만클리닉") != -1:
                    print(str(row) + "오류발생")
                    idx += 1
                    if(list_num < idx):
                        break;
                else:
                    break;
            if(list_num < idx):
                break;
            news.click()
            try:
                alert = driver.switch_to.alert
                alert.accept()
                time.sleep(2)
            except :
                pass
            try :
                time.sleep(1)
                name = driver.find_element_by_css_selector("h1#news_title_text_id").text
                date = driver.find_element_by_css_selector("p#date_text").text.split(" ")[1]
                parts = driver.find_elements_by_css_selector("div.par p")

                year, month, day = map(int, date.split('.'))
                date3 = datetime.date(year, month, day)

                for part in parts:
                    if part.text.find("기자") == -1 and part.text.replace(" ","") != "":
                        content += part.text
                    else:
                        break
                # 해당 기사의 날짜가 사용자 지정 범위 내인 경우에만 csv파일에 작성
                # 시작 날짜보다 이전에 작성된 기사라면 크롤링 종료
                if date1 > date3:
                    print("finish")
                    return df

                if date1 <= date3 and date3 <= date2:
                    #wr.writerow([row, name, date, content])
                    # append
                    df = df.append({'name': name, 'date': date, 'content': content}, ignore_index=True)
                    print(str(row) + " " + name + " / " + str(date3))

                row +=1
                driver.back()
                driver.implicitly_wait(5)
                idx += 1
                if(list_num < idx):
                    break;
            except (AttributeError, NoSuchElementException, UnexpectedAlertPresentException):
                driver.back()
                idx += 1
                if(list_num < idx):
                    break;

        page_now = driver.find_element_by_css_selector("div.paginate ul.paginate_num span.listAct").text.split(" ")[0]
        print("page " + str(int(page_now)) + " complete")
        url = "http://health.chosun.com/list.html?menu=&more_menu=&more_smenu=&nowcode=1&type=&pn=" + str(int(page_now) + 1)
        driver.get(url)